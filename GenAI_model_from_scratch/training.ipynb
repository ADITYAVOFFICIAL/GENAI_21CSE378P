{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chest X-Ray (CSR) Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "import pickle  # For saving the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Vocabulary Helper Class\n",
    "# -------------------------------\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<unk>\": 3}\n",
    "        self.idx2word = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n",
    "        self.count = 4\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split():\n",
    "            if word not in self.word2idx:\n",
    "                self.word2idx[word] = self.count\n",
    "                self.idx2word[self.count] = word\n",
    "                self.count += 1\n",
    "\n",
    "    def numericalize(self, sentence, max_len=50):\n",
    "        tokens = sentence.split()\n",
    "        tokens = [\"<start>\"] + tokens + [\"<end>\"]\n",
    "        token_ids = [self.word2idx.get(token, self.word2idx[\"<unk>\"]) for token in tokens]\n",
    "        if len(token_ids) < max_len:\n",
    "            token_ids += [self.word2idx[\"<pad>\"]] * (max_len - len(token_ids))\n",
    "        else:\n",
    "            token_ids = token_ids[:max_len]\n",
    "        return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Dataset Class for CXR Report Generation\n",
    "# -------------------------------\n",
    "class CXRReportDataset(Dataset):\n",
    "    def __init__(self, csv_file, vocab, image_root, transform=None, max_len=50, \n",
    "                 report_column='text', image_column='path'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to CSV file containing report and image file path columns.\n",
    "            vocab (Vocabulary): Vocabulary object to process reports.\n",
    "            image_root (str): Root directory for image files.\n",
    "            transform (callable, optional): Transformations for the images.\n",
    "            max_len (int): Maximum token length for reports.\n",
    "            report_column (str): The name of the column containing the report text.\n",
    "            image_column (str): The name of the column containing the image file path.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.vocab = vocab\n",
    "        self.image_root = image_root\n",
    "        self.transform = transform\n",
    "        self.max_len = max_len\n",
    "        self.report_column = report_column\n",
    "        self.image_column = image_column\n",
    "\n",
    "        # Define the prefix to remove from CSV paths if present.\n",
    "        self.csv_prefix = \"../input/curated-cxr-report-generation-dataset/mimic_dset/re_512_3ch/\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path_raw = row[self.image_column]\n",
    "        \n",
    "        # If the image path starts with the known CSV prefix, remove it and join with image_root\n",
    "        if isinstance(image_path_raw, str) and image_path_raw.startswith(self.csv_prefix):\n",
    "            relative_path = image_path_raw.replace(self.csv_prefix, \"\")\n",
    "            image_path = os.path.join(self.image_root, relative_path)\n",
    "        else:\n",
    "            image_path = image_path_raw\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        report = row[self.report_column]\n",
    "        report_ids = self.vocab.numericalize(report, self.max_len)\n",
    "        report_ids = torch.tensor(report_ids, dtype=torch.long)\n",
    "        return image, report_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Encoder-Decoder Model for Report Generation\n",
    "# -------------------------------\n",
    "class CXRReportGenerator(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(CXRReportGenerator, self).__init__()\n",
    "        # Encoder: Pretrained ResNet-50 (using resnet34 here for this code version as per earlier instructions)\n",
    "        resnet = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "        self.encoder = nn.Sequential(*list(resnet.children())[:-1])  # Output: (batch, 512, 1, 1)\n",
    "        self.fc = nn.Linear(512, hidden_size)  # Map image features to hidden size\n",
    "\n",
    "        # Decoder\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        # Encode images\n",
    "        features = self.encoder(images)             # (batch, 512, 1, 1)\n",
    "        features = features.view(features.size(0), -1)  # (batch, 512)\n",
    "        features = self.fc(features)                # (batch, hidden_size)\n",
    "        \n",
    "        # Get embeddings for captions\n",
    "        embeddings = self.embed(captions)           # (batch, seq_len, embed_size)\n",
    "        \n",
    "        # Use image features as the initial hidden state for the LSTM\n",
    "        h0 = features.unsqueeze(0)                  # (1, batch, hidden_size)\n",
    "        c0 = torch.zeros_like(h0)                   # (1, batch, hidden_size)\n",
    "        \n",
    "        outputs, _ = self.lstm(embeddings, (h0, c0))  # (batch, seq_len, hidden_size)\n",
    "        outputs = self.fc_out(outputs)              # (batch, seq_len, vocab_size)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Training Function with Accuracy Calculation\n",
    "# -------------------------------\n",
    "def train_model(model, dataloader, criterion, optimizer, device, epochs=10):\n",
    "    model.train()\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    pad_idx = 0  # <pad> token index\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        running_tokens = 0\n",
    "        for images, captions in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # Input captions except the last token; target is captions shifted by one.\n",
    "            outputs = model(images, captions[:, :-1])\n",
    "            loss = criterion(outputs.reshape(-1, outputs.size(2)), captions[:, 1:].reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy (token-level) for this batch\n",
    "            target = captions[:, 1:]\n",
    "            preds = outputs.argmax(dim=2)\n",
    "            mask = target != pad_idx\n",
    "            correct = (preds == target) & mask\n",
    "            running_correct += correct.sum().item()\n",
    "            running_tokens += mask.sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        epoch_acc = running_correct / running_tokens if running_tokens > 0 else 0\n",
    "        loss_history.append(epoch_loss)\n",
    "        accuracy_history.append(epoch_acc)\n",
    "        print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc*100:.2f}%\")\n",
    "    return loss_history, accuracy_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Visualization Function for Loss and Accuracy\n",
    "# -------------------------------\n",
    "def plot_metrics(loss_history, accuracy_history):\n",
    "    epochs = range(1, len(loss_history) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss_history, marker='o', label='Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, accuracy_history, marker='o', label='Accuracy', color='green')\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Main Function\n",
    "# -------------------------------\n",
    "def main():\n",
    "    # Configurations and file paths\n",
    "    csv_train = '/kaggle/input/curated-cxr-report-generation-dataset/NLP_aug_datasets/df_train_aug.csv'\n",
    "    image_root = '/kaggle/input/curated-cxr-report-generation-dataset/mimic_dset/re_512_3ch'\n",
    "    num_epochs = 30\n",
    "    batch_size = 32\n",
    "    learning_rate = 1e-3\n",
    "    max_len = 50                         # Maximum length of report (in tokens)\n",
    "    embed_size = 256\n",
    "    hidden_size = 256\n",
    "    num_layers = 1\n",
    "\n",
    "    # Image transformations for CXR images\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Load the training CSV and print available columns for debugging\n",
    "    df_train = pd.read_csv(csv_train)\n",
    "    print(\"Columns in training CSV:\", df_train.columns.tolist())\n",
    "\n",
    "    # In this CSV, the report text is in the \"text\" column and the image path in the \"path\" column.\n",
    "    report_column = 'text'\n",
    "    image_column = 'path'\n",
    "\n",
    "    # Build vocabulary from the training CSV using the \"text\" column\n",
    "    vocab = Vocabulary()\n",
    "    for report in df_train[report_column]:\n",
    "        vocab.add_sentence(report)\n",
    "    vocab_size = vocab.count\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "    # Save the vocabulary to a pickle file for later use during inference\n",
    "    with open('vocab.pkl', 'wb') as f:\n",
    "        pickle.dump(vocab, f)\n",
    "    print(\"Vocabulary saved to vocab.pkl\")\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    train_dataset = CXRReportDataset(csv_train, vocab, image_root, transform, max_len,\n",
    "                                     report_column=report_column, image_column=image_column)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    # Device configuration: use GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = CXRReportGenerator(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "    \n",
    "    # Use DataParallel if multiple GPUs are available (e.g., dual T4 on Kaggle)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Multiple GPUs detected: {torch.cuda.device_count()}. Using DataParallel.\")\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx[\"<pad>\"])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model and obtain loss and accuracy histories\n",
    "    loss_history, accuracy_history = train_model(model, train_loader, criterion, optimizer, device, epochs=num_epochs)\n",
    "\n",
    "    # Visualize the training loss and accuracy\n",
    "    plot_metrics(loss_history, accuracy_history)\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), 'cxr_report_generator.pth')\n",
    "    print(\"Training complete. Model saved as cxr_report_generator.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
